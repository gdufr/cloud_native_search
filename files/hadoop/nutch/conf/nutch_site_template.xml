<?xml version="1.0"?>
<configuration>
    <property>
        <name>http.agent.name</name>
        <value>Search</value>
        <description>MUST NOT be empty. The advertised version will
        have Nutch appended.</description>
    </property>
    <property>
        <name>http.robots.agents</name>
        <value>Search,*</value>
        <description>The agent strings we'll look for in robots.txt
        files, comma-separated, in decreasing order of precedence. You
        should put the value of http.agent.name as the first agent
        name, and keep the default * at the end of the list. E.g.:
        BlurflDev,Blurfl,*. If you don't, your logfile will be full of
        warnings.</description>
    </property>
    <property>
        <name>fetcher.server.min.delay</name>
        <value>0</value>
        <description>The minimum number of seconds the fetcher will delay between
        successive requests to the same server. This value is applicable ONLY
        if fetcher.threads.per.queue is greater than 1 (i.e. the host blocking
        is turned off).</description>
    </property>
    <property>
        <name>fetcher.threads.per.queue</name>
        <value>12</value>
        <description>This number is the maximum number of threads that
        should be allowed to access a queue at one time. Setting it to
        a value > 1 will cause the Crawl-Delay value from robots.txt to
        be ignored and the value of fetcher.server.min.delay to be used
        as a delay between successive requests to the same server instead
        of fetcher.server.delay.
        </description>
    </property>
    <property>
        <name>fetcher.threads.fetch</name>
        <value>6</value>
        <description>The number of FetcherThreads the fetcher should use.
        This is also determines the maximum number of requests that are
        made at once (each FetcherThread handles one connection). The total
        number of threads running in distributed mode will be the number of
        fetcher threads * number of nodes as fetcher has one map task per node.
        </description>
    </property>
    <property>
        <name>fetcher.server.delay</name>
        <value>0</value>
    </property>
    <property>
        <name>http.content.limit</name>
        <value>-1</value>
    </property>
    <property>
      <name>db.fetch.interval.default</name>
      <value>80000</value>
      <description>The default number of seconds between re-fetches of a page
        86400 = 1 day
        Set to 80000 to force recrawl daily 
      </description>
    </property>
    <property>
        <name>db.ignore.external.links</name>
        <value>true</value>
        <description>If true, outlinks leading from a page to external
        hosts will be ignored. This is an effective way to limit the
        crawl to include only initially injected hosts, without
        creating complex URLFilters.</description>
    </property>
    <property>
        <name>db.max.outlinks.per.page</name>
        <value>-1</value>
    </property>
    <property>
        <name>plugin.includes</name>
        <value>protocol-http|urlfilter-regex|parse-(html|tika|metatags)|index-(basic|anchor|metadata)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|indexer-cloudsearch|urlnormalizer-(pass|regex|basic)</value>
        <description></description>
    </property>
    <property>
        <name>metatags.names</name>
        <value>author,charset,country,description,keywords,language,referrer,robots,snippet,title</value>
        <description>Names of the metatags to extract, separated by
        ','. Prefixes the names with 'metatag.' in the parse-metadata. For instance to index
        description and keywords, you need to activate the plugin
        index-metadata and set the value of the parameter 'index.parse.md' to
        'metatag.description,metatag.keywords'.</description>
    </property>
    <property>
        <name>index.parse.md</name>
        <value>metatag.author,metatag.charset,metatag.country,metatag.description,metatag.keywords,metatag.language,metatag.referrer,metatag.robots,metatag.snippet,metatag.title</value>
        <description>Comma-separated list of keys to be taken from the
        parse metadata to generate fields. Can be used e.g. for
        'description' or 'keywords' provided that these values are
        generated by a parser (see parse-metatags plugin)</description>
    </property>
    <property>
        <name>cloudsearch.batch.maxSize</name>
        <value>500</value>
    </property>
    <property>
        <name>cloudsearch.endpoint</name>
        <value>${cs_doc_endpoint}</value>
    </property>
    <property>
        <name>cloudsearch.region</name>
        <value>${cs_doc_region}</value>
    </property>
</configuration>
